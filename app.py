import streamlit as st
import pandas as pd
import numpy as np
from train_model import FakeNewsDetector, train_and_save_model
import joblib
import os
from langdetect import detect
import matplotlib.pyplot as plt
import seaborn as sns
import torch

# V√©rifier si le module transformer_model est disponible
try:
    from transformer_model import TransformerFakeNewsDetector
    TRANSFORMER_AVAILABLE = True
except ImportError:
    TRANSFORMER_AVAILABLE = False

# Configuration de la page
st.set_page_config(
    page_title="D√©tecteur de Fake News",
    page_icon="üì∞",
    layout="wide",
    menu_items={
        'About': "D√©tecteur de Fake News - Projet de Fouille de Donn√©es"
    }
)

# Configuration de matplotlib pour √©viter les probl√®mes DOM
st.set_option('deprecation.showPyplotGlobalUse', False)

# Titre de l'application
st.title("D√©tecteur de Fake News üì∞")

# Fonction pour d√©tecter la langue du texte
def detect_language(text):
    try:
        lang = detect(text)
        if lang == 'fr':
            return "Fran√ßais", "fr"
        else:
            return "Anglais", "en"  # Par d√©faut, on consid√®re que c'est de l'anglais
    except:
        return "Anglais", "en"  # En cas d'erreur, on d√©faut sur l'anglais
        
# Fonction pour charger les datasets et calculer les statistiques
def load_datasets():
    stats = {}
    
    # Charger les donn√©es anglaises
    try:
        df_fake_en = pd.read_csv('Fake.csv')
        df_true_en = pd.read_csv('True.csv')
        english_data = pd.concat([df_fake_en, df_true_en])
        english_data['is_fake'] = [True] * len(df_fake_en) + [False] * len(df_true_en)
        
        stats['en'] = {
            'total': len(english_data),
            'fake': len(df_fake_en),
            'true': len(df_true_en),
            'loaded': True
        }
        
        if 'subject' in english_data.columns:
            stats['en']['subjects'] = english_data['subject'].value_counts().to_dict()
    except Exception as e:
        stats['en'] = {'loaded': False, 'error': str(e)}
    
    # Charger les donn√©es fran√ßaises
    try:
        df_fr = pd.read_csv('french dataset/train.csv', sep=';', encoding='utf-8')
        
        stats['fr'] = {
            'total': len(df_fr),
            'fake': len(df_fr[df_fr['fake'] == 1]),
            'true': len(df_fr[df_fr['fake'] == 0]),
            'loaded': True
        }
        
        # Statistiques suppl√©mentaires pour le dataset fran√ßais
        if 'media' in df_fr.columns:
            stats['fr']['sources'] = df_fr['media'].value_counts().to_dict()
    except Exception as e:
        stats['fr'] = {'loaded': False, 'error': str(e)}
    
    return stats

# Fonction pour charger ou entra√Æner le mod√®le
def get_model():
    try:
        detector = FakeNewsDetector.load('fake_news_model.joblib')
        st.success("Mod√®le charg√© avec succ√®s !")
    except:
        with st.spinner("Premi√®re utilisation : entra√Ænement du mod√®le en cours..."):
            # V√©rifier si les fichiers de donn√©es existent
            if not os.path.exists('Fake.csv') or not os.path.exists('True.csv'):
                # Charger les donn√©es depuis le repository
                fake_url = "https://raw.githubusercontent.com/Rimka33/detection-de-fake-news/main/Fake.csv"
                true_url = "https://raw.githubusercontent.com/Rimka33/detection-de-fake-news/main/True.csv"
                
                try:
                    df_fake = pd.read_csv(fake_url)
                    df_true = pd.read_csv(true_url)
                    
                    # Sauvegarder localement
                    df_fake.to_csv('Fake.csv', index=False)
                    df_true.to_csv('True.csv', index=False)
                except Exception as e:
                    st.error(f"Erreur lors du chargement des donn√©es : {str(e)}")
                    st.stop()
            
            # Entra√Æner le mod√®le
            train_and_save_model()
            detector = FakeNewsDetector.load('fake_news_model.joblib')
            st.success("Mod√®le entra√Æn√© et charg√© avec succ√®s !")
    
    return detector

# Charger les statistiques des datasets
dataset_stats = load_datasets()

# Interface de l'application avec onglets
tab1, tab2, tab3 = st.tabs(["üìù Analyser un article", "üìä Statistiques des datasets", "‚ÑπÔ∏è √Ä propos"])

try:
    # Charger le mod√®le
    detector = get_model()
    
    # V√©rifier si un mod√®le transformer est disponible
    transformer_detector = None
    if TRANSFORMER_AVAILABLE and os.path.exists('transformer_models/camembert_fake_news/metadata.joblib'):
        try:
            transformer_detector = TransformerFakeNewsDetector.load('transformer_models/camembert_fake_news')
            st.sidebar.success("Mod√®le transformer CamemBERT charg√© !")
        except Exception as e:
            st.sidebar.warning(f"Impossible de charger le mod√®le transformer: {str(e)}")
    
    if detector is not None:
        # Onglet 1: Analyser un article
        with tab1:
            st.header("üìù Analyser un article")
            
            # S√©lection du mod√®le √† utiliser
            model_type = st.radio(
                "Mod√®le √† utiliser",
                ["Clustering (K-means)"] + (["Transformer (CamemBERT)"] if transformer_detector else []),
                horizontal=True
            )
            
            # Formulaire de saisie
            with st.form("news_form"):
                title = st.text_input("Titre de l'article")
                text = st.text_area("Contenu de l'article")
                submitted = st.form_submit_button("Analyser")
            
            if submitted and text:  # Le titre peut √™tre optionnel
                with st.spinner("Analyse en cours..."):
                    # D√©tecter automatiquement la langue
                    detected_language, lang_code = detect_language(text)
                    st.info(f"Langue d√©tect√©e : {detected_language}")
                    
                    # Pr√©diction selon le mod√®le choisi
                    if "Transformer" in model_type and transformer_detector:
                        result = transformer_detector.predict_one(text if not title else f"{title}. {text}")
                    else:
                        # Utiliser le mod√®le de clustering par d√©faut
                        result = detector.predict_one(title if title else text[:50], text)
                    
                    # Affichage des r√©sultats
                    st.header("üìä R√©sultats de l'analyse")
                    
                    # Affichage du verdict
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        if result['is_fake']:
                            st.error("‚ö†Ô∏è Cet article est probablement une FAKE NEWS")
                        else:
                            st.success("‚úÖ Cet article semble fiable")
                    
                    with col2:
                        st.metric("Indice de confiance", f"{result['confidence']*100:.1f}%")
                    
                    # D√©tails techniques
                    with st.expander("D√©tails techniques"):
                        st.write(f"Langue d√©tect√©e : {detected_language}")
                        if "Transformer" not in model_type:
                            st.write(f"Cluster assign√© : {result['cluster']}")
                            st.write(f"Z-score : {result['z_score']:.2f}")
                            st.write("Note : Un Z-score > 2.0 indique une anomalie potentielle")
        
        # Onglet 2: Statistiques des datasets
        with tab2:
            st.header("üìä Statistiques des datasets d'entra√Ænement")
            
            # Affichage des statistiques dans deux colonnes
            col1, col2 = st.columns(2)
            
            # Dataset Anglais
            with col1:
                st.subheader("Dataset Anglais")
                if dataset_stats['en']['loaded']:
                    # M√©triques de base
                    st.metric("Nombre total d'articles", dataset_stats['en']['total'])
                    st.metric("Articles vrais", dataset_stats['en']['true'])
                    st.metric("Articles faux", dataset_stats['en']['fake'])
                    
                    # Distribution des sujets si disponible
                    if 'subjects' in dataset_stats['en']:
                        st.write("\nDistribution des sujets :")
                        subjects_df = pd.DataFrame(
                            list(dataset_stats['en']['subjects'].items()), 
                            columns=['Sujet', 'Nombre']
                        ).sort_values('Nombre', ascending=False)
                        
                        st.dataframe(subjects_df)
                        
                        # Afficher le top 5 des sujets
                        st.write("Top 5 des sujets les plus fr√©quents :")
                        st.table(subjects_df.head(5))
                else:
                    st.warning("‚ö†Ô∏è Impossible de charger les donn√©es anglaises")
                    if 'error' in dataset_stats['en']:
                        st.error(dataset_stats['en']['error'])
            
            # Dataset Fran√ßais
            with col2:
                st.subheader("Dataset Fran√ßais")
                if dataset_stats['fr']['loaded']:
                    # M√©triques de base
                    st.metric("Nombre total d'articles", dataset_stats['fr']['total'])
                    st.metric("Articles vrais", dataset_stats['fr']['true'])
                    st.metric("Articles faux", dataset_stats['fr']['fake'])
                    
                    # Affichage des sources si disponible
                    if 'sources' in dataset_stats['fr']:
                        st.write("\nDistribution des sources :")
                        sources_df = pd.DataFrame(
                            list(dataset_stats['fr']['sources'].items()), 
                            columns=['Source', 'Nombre']
                        ).sort_values('Nombre', ascending=False)
                        
                        st.dataframe(sources_df)
                        
                        # Afficher le top 5 des sources
                        st.write("Top 5 des sources les plus fr√©quentes :")
                        st.table(sources_df.head(5))
                else:
                    st.warning("‚ö†Ô∏è Impossible de charger les donn√©es fran√ßaises")
                    if 'error' in dataset_stats['fr']:
                        st.error(dataset_stats['fr']['error'])
        
        # Onglet 3: √Ä propos
        with tab3:
            st.header("‚ÑπÔ∏è √Ä propos de cette application")
            st.write("Cette application utilise l'apprentissage automatique pour d√©tecter les fake news.")
            st.write("Le mod√®le a √©t√© entra√Æn√© sur des datasets en anglais et en fran√ßais.")
            st.write("La langue de l'article est d√©tect√©e automatiquement.")
            
            st.subheader("Mod√®les disponibles")
            st.markdown("""
            1. **Mod√®le de clustering (K-means)**
               - Utilise TF-IDF pour extraire les caract√©ristiques du texte
               - Regroupe les articles en clusters et d√©tecte les anomalies
               - Rapide et efficace pour le d√©ploiement
            
            2. **Mod√®les Transformer**
               - CamemBERT: Sp√©cialement entra√Æn√© pour le fran√ßais
               - FlauBERT: Alternative fran√ßaise √† BERT
               - BERT: Mod√®le multilingue pr√©-entra√Æn√©
               - RoBERTa: Version optimis√©e de BERT
               - DistilBERT: Version l√©g√®re de BERT pour le d√©ploiement
            """)
            
            st.subheader("Datasets utilis√©s")
            st.markdown("""
            - **Dataset anglais**: Articles de diff√©rentes sources, class√©s en vrais et faux
            - **Dataset fran√ßais**: Articles adapt√©s au contexte francophone
            """)
            
            with st.expander("Informations techniques d√©taill√©es"):
                st.write("Le mod√®le de clustering utilise une approche non supervis√©e avec K-means.")
                st.write("Les caract√©ristiques sont extraites √† l'aide de TF-IDF sur le texte pr√©trait√©.")
                st.write("La d√©tection se base sur la distance aux centro√Ødes et les ratios de fake news par cluster.")
                
                if TRANSFORMER_AVAILABLE:
                    st.write("\n**Mod√®les Transformer:**")
                    st.write("Ces mod√®les utilisent l'architecture Transformer pour comprendre le contexte et les nuances du langage.")
                    st.write("Ils sont fine-tun√©s sur notre dataset de fake news pour s'adapter √† cette t√¢che sp√©cifique.")
                    st.write("Les mod√®les fran√ßais comme CamemBERT et FlauBERT sont particuli√®rement adapt√©s pour les textes en fran√ßais.")

    else:
        st.warning("‚ö†Ô∏è Le mod√®le n'est pas charg√©. Veuillez ex√©cuter train_model.py d'abord.")

except Exception as e:
    st.error(f"Erreur : {str(e)}")
    st.info("Si vous rencontrez des probl√®mes, essayez d'ex√©cuter l'application en local.")
